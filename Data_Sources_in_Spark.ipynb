{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e4af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set spark environments\n",
    "os.environ['PYSPARK_PYTHON'] = r'C:\\Users\\room102sys2\\AppData\\Local\\Programs\\Python\\Python39'\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\\Users\\room102sys2\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\jupyter'\n",
    "#os.environ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efdd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b96017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\Program Files\\Spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee613153",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SparkSession.builder.appName(\"Data Sources\").getOrCreate()\n",
    "sc = sp.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0229a",
   "metadata": {},
   "source": [
    "Spark has Six core data sources and hundreds of external data sources written by the community. The ability to read and write from all different kinds of data sources and for the community to create its own contributions is one of Sparks greatest strength.\n",
    "Following are Spark's core data sources:\n",
    "- CSV\n",
    "- JSON\n",
    "- Parquet\n",
    "- ORC\n",
    "- JDBC/ODBC connections\n",
    "- Plain-text files\n",
    "\n",
    "And Spark has numerous community-created data sources. Few of them are mentioned here:\n",
    "\n",
    "- Cassandra\n",
    "- HBase\n",
    "- MongoDB\n",
    "- AWS Redshift\n",
    "- XML\n",
    "- And many others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5601d45",
   "metadata": {},
   "source": [
    "## Read API Structure\n",
    "The core structure for reading data is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b670e93",
   "metadata": {},
   "source": [
    "`DataFrameReader.format(...).option(\"key\",\"value\").schema(...).load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe11474",
   "metadata": {},
   "source": [
    "- `format` is optional because by default Spark will Use the Parquet format.\n",
    "- `option` allows us to key-value configurations to parameterize how we read data.\n",
    "- `schema` is optional if the data source provides a schema or if you intended to use schema inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19afd56",
   "metadata": {},
   "source": [
    "## Basics of Reading Data\n",
    "The basic thing for reading data in Spark is the `DataFrameReader`. We access this through the `SparkSession` via the read attribute:\n",
    "\n",
    "`spark.read`\n",
    "\n",
    "After we have a DataFrame reader, we specify several values:\n",
    "\n",
    "- The `format`\n",
    "- The `schema`\n",
    "- The *read mode*\n",
    "- A series of *`options`*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb70708",
   "metadata": {},
   "source": [
    "- The format options, and schema each return a `DataFrameReader` that can undergo further tranformations and are all optional, except for one option.\n",
    "- Each data source has a specific set of options that determine how the data is read into spark.\n",
    "- At a minimum, we must supply the `DataFrameReader` a path to from which to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561dd254",
   "metadata": {},
   "source": [
    "Here is the example of the overall layout:\n",
    "```\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"mode\",\"FAILFAST\")\n",
    "  .option(\"path\",\"path/to/file(s)\")\n",
    "  .schema(someSchema)\n",
    "  .load()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238db15",
   "metadata": {},
   "source": [
    "### Read modes\n",
    "- Reading data from an external source naturally entails encountring malformed data, especially when working with only semi-structured data sources.\n",
    "- Read modes specify what will happen when Spark does come across malformed records.\n",
    "\n",
    "\n",
    "| Read mode       | Description                                                                                             |  \n",
    "|-----------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `permissive`    | Sets all fields to `null` when it encounters a corrupted record and places all corrupted records in a   |   \n",
    "|                  |string column `called_corrupt_record`                                                                   | \n",
    "| `dropMalformed` | Drops the row that contains malformed records                                                           |   \n",
    "| `failFast`      | Fails immediatley upon encountring malformed records                                                    |   |-----------------|---------------------------------------------------------------------------------------------------------|\n",
    "\n",
    "The default is `permissive`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282fb9d",
   "metadata": {},
   "source": [
    "### Write API Structure\n",
    "The core structure for writing data is as follows:\n",
    "`DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e391d",
   "metadata": {},
   "source": [
    "- `format` is optional because by default, spark will use the parquet format. \n",
    "- `option` allows us to configure how to write out our given data.\n",
    "- `PartitionBy`, `bucketBy`, and `sortBy` work only for file-based data sources; We can use them to control the specific layout of files at the destinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc28f9a",
   "metadata": {},
   "source": [
    "## Basics of Writing Data\n",
    "\n",
    "- For writing data we use `DataFrameWriter`. \n",
    "- After we have a `DataFrameWriter`, we specify three values: the `format`, a series of `options`, and the `save` mode. \n",
    "- At a minimum, we must supply a path.\n",
    "\n",
    "Here is the syntax for writing data:\n",
    "```\n",
    "dataframe.write.format(\"csv\")\n",
    "    .option(\"mode\",\"OVERWRITE\")\n",
    "    .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "    .option(\"path\",\"path/to/file(s)\")\n",
    "    .save()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f17227",
   "metadata": {},
   "source": [
    "### Save modes\n",
    "Save modes specify what will happen if Spark finds data at the specified location. Following table lists the save modes:\n",
    "\n",
    "| Save mode       | Description                                                                                  |\n",
    "|-----------------|----------------------------------------------------------------------------------------------|\n",
    "| `append`        | Appends the output files to the list of files that already exist at that location.           |\n",
    "| `overwrite`     | will completely overwrite any data that already exists there.                                |\n",
    "| `errorIfExists` | Throws an error and fails the write if data or files already exist at the specified location |\n",
    "| `ignore`        | If data or files exist at the location, do nothing with the current DataFrame                |\n",
    "\n",
    "The default is `errorIfExists`.This means that if spark finds data at the location to which we are writing, it will fail the write immediatley."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd924a",
   "metadata": {},
   "source": [
    "## CSV Files\n",
    "- CSV stands for comma-separated values.\n",
    "### CSV Options\n",
    "\n",
    "| read/write| key                  | Potential values           | Default  | Description                                   | \n",
    "|-----------|----------------------|----------------------------|----------|-----------------------------------------------|\n",
    "| Both      | compression or codec |None,uncompressed,          |          |Declares what compression codec Spark          |\n",
    "|           |                      |bzip2, deflate,             | none     |                                               |\n",
    "| Both      | dateFormat           |Any string orcharacter that |          |                                               |\n",
    "|           |                      |conforms to Java’s          |          |                                               |\n",
    "|           |                      |SimpleDataFormat            |yyyy-MM-dd| \n",
    "| `errorIfExists` | Throws an error and fails the write if data or files already exist at the specified location |\n",
    "| `ignore`        | If data or files exist at the location, do nothing with the current DataFrame                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb7ea",
   "metadata": {},
   "source": [
    "### Reading CSV Files\n",
    "\n",
    "- To read CSV file first we need to create a `DataFrameReader`. Here is the syntax:\n",
    "\n",
    "`spark.read.format(\"csv\")`\n",
    "\n",
    "Here is the expanded syntax:\n",
    "\n",
    "`spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"mode\",\"FAILFAST\")\n",
    "    .option(\"inferschema\",\"true\")\n",
    "    .load(\"some/path/to/file.csv\")`\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9637d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False)\n",
    "])\n",
    "\n",
    "sp.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema).load(\"F:/proj_pyspark/data/Spark-The-Definitive-Guide/data/flight-data/csv/2010-summary.csv\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6d9cc",
   "metadata": {},
   "source": [
    "If you give incorrect data type of spark schema. The problem mainfest itself when Spark actually reads the data. As soon as we start our Spark job, it will immediatley fail due to the data not confirming to the specified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e1bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "myManualSchema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", LongType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", LongType(), True),\n",
    "    StructField(\"count\", LongType(), False) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a379fc6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o44.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (room102csesys9 executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:400)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:309)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:254)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:396)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 22 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"United States\"\r\n\tat java.lang.NumberFormatException.forInputString(Unknown Source)\r\n\tat java.lang.Long.parseLong(Unknown Source)\r\n\tat java.lang.Long.parseLong(Unknown Source)\r\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\r\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\r\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:238)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:291)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ROOM10~1\\AppData\\Local\\Temp/ipykernel_8880/2851462808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mode\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FAILFAST\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyManualSchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F:/proj_pyspark/data/Spark-The-Definitive-Guide/data/flight-data/csv/2010-summary.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o44.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (room102csesys9 executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:400)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:309)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:254)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:396)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 22 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"United States\"\r\n\tat java.lang.NumberFormatException.forInputString(Unknown Source)\r\n\tat java.lang.Long.parseLong(Unknown Source)\r\n\tat java.lang.Long.parseLong(Unknown Source)\r\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\r\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\r\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:238)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:160)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:291)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "sp.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema).load(\"F:/proj_pyspark/data/Spark-The-Definitive-Guide/data/flight-data/csv/2010-summary.csv\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8019b3",
   "metadata": {},
   "source": [
    "- In genral Spark will fail only at job execution time rather than DataFrame dafinition time-----this is called __lazy evaluation__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da25d9",
   "metadata": {},
   "source": [
    "### Writing CSV Files\n",
    "Lets see the simple example for writing data after reading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a465d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = sp.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"F:/proj_pyspark/data/Spark-The-Definitive-Guide/data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801800b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:332)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:402)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ROOM10~1\\AppData\\Local\\Temp/ipykernel_8880/1888656152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcsvFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sep\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my-tsv-file.tsv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:332)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:402)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820579a",
   "metadata": {},
   "source": [
    "When we list the destination directory, you can see that `my-tsv-file` is actually a folder with numerous\n",
    "files within it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd435e2",
   "metadata": {},
   "source": [
    "## JSON Files\n",
    "- In Spark, when we refer to JSON files, we refer to linedelimited JSON files.\n",
    "- The line-delimited versus multiline trade-off is controlled by a single option: multiLine.\n",
    "- Line-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record (rather   than having to read in an entire file and then write it out), which is what we recommend that you use.\n",
    "- The line-delimited versus multiline trade-off is controlled by a single option: multiLine. When you set this option to true,   you can read an entire file as one json object and Spark will go through the work of parsing that into a DataFrame.\n",
    "- Line-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record (rather   than having to read in an entire file and then write it out), which is what we recommend that you use.\n",
    "- Another key reason for the popularity of line-delimited JSON is because JSON objects have structure, and JavaScript (on which\n",
    "  JSON is based) has at least basic types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055f48a",
   "metadata": {},
   "source": [
    "### JSON Options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a079261",
   "metadata": {},
   "source": [
    "### Reading JSON Files\n",
    "Lets see an example of reading JSON file and its options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "sp.read.format(\"json\").option(\"mode\", \"FAILFAST\").option(\"inferSchema\", \"true\").load(\"./data/Spark-The-Definitive-Guide/data/flight-data/json/2010-summary.json\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc7564",
   "metadata": {},
   "source": [
    "### Writing JSON Files\n",
    "- Writing JSON files is just as simple as reading them, and, as you might expect, the data source does not matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea7df4",
   "metadata": {},
   "source": [
    "## Parquet Files\n",
    "- Parquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads.\n",
    "- It provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files.\n",
    "- It is a file format that works exceptionally well with Apache Spark and is in fact the default file format.\n",
    "- Another advantage of Parquet is that it supports complex types. This means that if your column is an array (which would fail with a CSV file, for example), map, or struct, you’ll still be able to read and write that file without issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79932369",
   "metadata": {},
   "source": [
    "Syntax:\n",
    "\n",
    "`spark.read.format(\"parquet\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651f481",
   "metadata": {},
   "source": [
    "### Reading Parquet Files\n",
    "- Parquet has very few options because it enforces its own schema when storing data.\n",
    "- We can set the schema if we have strict requirements for what our DataFrame should look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24188dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.read.format(\"parquet\").load(\"./data/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e91b8f",
   "metadata": {},
   "source": [
    "### Parquet options\n",
    " \n",
    "| Read/Write | Key                 | Potential Values   | Default                     |   Description                         |\n",
    "|------------|---------------------|--------------------|-----------------------------|---------------------------------------|\n",
    "|Write       |compression or codec |None, uncompressed, | None                        | Declares what compression codec       |\n",
    "|            |                     |bzip2, deflate,     |                             |Spark should use to read or write the  |\n",
    "|            |                     |gzip, lz4, or snappy|                             |file.                                  |\n",
    "|Read        |mergeSchema          |true, false         |Value of the configuration   |You can incrementally add columns to   |\n",
    "|            |                     |                    |spark.sql.parquet.mergeSchema|newly written Parquet files in the same|\n",
    "|            |                     |                    |                             |table/folder. Use this option to enable|\n",
    "|            |                     |                    |                             | disable this feature.                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a5c70",
   "metadata": {},
   "source": [
    "### Writing Parquet Files\n",
    "Writing Parquet is as easy as reading it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0ffdc",
   "metadata": {},
   "source": [
    "## ORC Files\n",
    "- ORC is a self-describing, type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly.\n",
    "- ORC actually has no options for reading in data because Spark understands the file format quite well.\n",
    "- The fundamental difference is that Parquet is further optimized for use with Spark, whereas ORC is further optimized for Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb403877",
   "metadata": {},
   "source": [
    "### Reading Orc Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "sp.read.format(\"orc\").load(\"./data/Spark-The-Definitive-Guide/data/flight-data/orc/2010-summary.orc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01d792",
   "metadata": {},
   "source": [
    "### Writing Orc Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa38165",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-json-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dc3e4",
   "metadata": {},
   "source": [
    "## SQL Databases\n",
    "\n",
    "- SQL datasources are one of the more powerful connectors because there are a variety of systems to which you can connect (as long as that system speaks SQL).\n",
    "- To read and write from these databases, you need to do two things: include the Java Database Connectivity (JDBC) driver for you particular database on the spark classpath, and provide the proper JAR for the driver itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2c136",
   "metadata": {},
   "source": [
    "| Property Name | Meaning |  |\n",
    "|---|---|---|\n",
    "| url | The JDBC URL to which to connect. The source-specific connection properties can be specified in<br>the URL; for example, jdbc:postgresql://localhost/test?user=fred&password=secret. |  |\n",
    "| dbtable | The JDBC table to read. Note that anything that is valid in a FROM clause of a SQL query can be<br>used. For example, instead of a full table you could also use a subquery in parentheses. |  |\n",
    "| driver | The class name of the JDBC driver to use to connect to this URL. |  |\n",
    "| partitionColumn,<br>lowerBound, upperBound | If any one of these options is specified, then all others must be set as well. In addition,<br>numPartitions must be specified. These properties describe how to partition the table when reading<br>in parallel from multiple workers. partitionColumn must be a numeric column from the table in<br>question. Notice that lowerBound and upperBound are used only to decide the partition stride, not for<br>filtering the rows in the table. Thus, all rows in the table will be partitioned and returned. This option<br>applies only to reading. |  |\n",
    "| numPartitions | The maximum number of partitions that can be used for parallelism in table reading and writing. This<br>also determines the maximum number of concurrent JDBC connections. If the number of partitions<br>to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before<br>writing. |  |\n",
    "| fetchsize | The JDBC fetch size, which determines how many rows to fetch per round trip. This can help<br>performance on JDBC drivers, which default to low fetch size (e.g., Oracle with 10 rows). This<br>option applies only to reading. |  |\n",
    "| batchsize | The JDBC batch size, which determines how many rows to insert per round trip. This can help<br>performance on JDBC drivers. This option applies only to writing. The default is 1000. |  |\n",
    "| isolationLevel | The transaction isolation level, which applies to current connection. It can be one of NONE,<br>READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to<br>standard transaction isolation levels defined by JDBC’s Connection object. The default is<br>READ_UNCOMMITTED. This option applies only to writing. For more information, refer to the<br>documentation in java.sql.Connection. |  |\n",
    "| truncate | This is a JDBC writer-related option. When SaveMode.Overwrite is enabled, Spark truncates an<br>existing table instead of dropping and re-creating it. This can be more efficient, and it prevents the<br>table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as<br>when the new data has a different schema. The default is false. This option applies only to writing. |  |\n",
    "| createTableOptions | This is a JDBC writer-related option. If specified, this option allows setting of database-specific table<br>and partition options when creating a table (e.g., CREATE TABLE t (name string)<br>ENGINE=InnoDB). This option applies only to writing. |  |\n",
    "| createTableColumnTypes | The database column data types to use instead of the defaults, when creating the table. Data type<br>information should be specified in the same format as CREATE TABLE columns syntax (e.g.,<br>“name CHAR(64), comments VARCHAR(1024)”). The specified types should be valid Spark SQL<br>data types. This option applies only to writing. |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75552f",
   "metadata": {},
   "source": [
    "### Reading from SQL Databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"data/Spark-The-Definitive-Guide/data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "dbDataFrame = sp.read.format(\"jdbc\").option(\"url\", url)\\\n",
    ".option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e781b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "pgDF = sp.read.format(\"jdbc\")\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    ".option(\"dbtable\", \"schema.tablename\")\\\n",
    ".option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4803df1",
   "metadata": {},
   "source": [
    "## Advanced I/O Concepts\n",
    "We can also control specific data layout by controlling two things: bucketing and partitioning\n",
    "\n",
    "### Splittable File Types and Compression\n",
    "- Certain file formats are fundamentally “splittable.” This can improve speed because it makes it possible for Spark to avoid reading an entire file, and access only the parts of the file necessary to satisfy your query.\n",
    "- Additionally if you’re using something like Hadoop Distributed File System (HDFS), splitting a file can provide further optimization if that file spans multiple blocks.\n",
    "\n",
    "### Reading Data in Parallel\n",
    "- Multiple executors cannot read from the same file at the same time necessarily, but they can read different files at the same time.\n",
    "- In general, this means that when you read from a folder with multiple files in it, each one of those files will become a partition in your DataFrame and be read in by available executors in parallel (with the remaining queueing up behind the others).\n",
    "\n",
    "### Writing Data in Parallel\n",
    "- The number of files or data written is dependent on the number of partitions the DataFrame has at the time you write out the data.\n",
    "- By default, one file is written per partition of the data.\n",
    "\n",
    "Example:\n",
    "`csvFile.repartition(5).write.format(\"csv\").save(\"/tmp/multiple.csv\")`\n",
    "\n",
    "ls /tmp/multiple.csv\n",
    "\n",
    "/tmp/multiple.csv/part-00000-767df509-ec97-4740-8e15-4e173d365a8b.csv\n",
    "\n",
    "/tmp/multiple.csv/part-00001-767df509-ec97-4740-8e15-4e173d365a8b.csv\n",
    "\n",
    "/tmp/multiple.csv/part-00002-767df509-ec97-4740-8e15-4e173d365a8b.csv\n",
    "\n",
    "/tmp/multiple.csv/part-00003-767df509-ec97-4740-8e15-4e173d365a8b.csv\n",
    "\n",
    "/tmp/multiple.csv/part-00004-767df509-ec97-4740-8e15-4e173d365a8b.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef856856",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "- Partitioning is a tool that allows you to control what data is stored (and where) as you write it. When you write a file to a partitioned directory (or table), you basically encode a column as a folder. What this allows you to do is skip lots of data when you go to read it in later, allowing you to read in only the data relevant to your problem instead of having to scan the complete dataset. These are supported \n",
    "\n",
    "for all file-based data sources:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
