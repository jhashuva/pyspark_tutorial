{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1326cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set spark environments\n",
    "os.environ['PYSPARK_PYTHON'] = r'C:\\Users\\room102sys2\\AppData\\Local\\Programs\\Python\\Python39'\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\\Users\\room102sys2\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\jupyter'\n",
    "#os.environ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2770bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f146dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\Program Files\\Spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779fbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SparkSession.builder.appName(\"SparkQL\").getOrCreate()\n",
    "sc = sp.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192db762",
   "metadata": {},
   "source": [
    "# RESILENT DISTRIBUTED DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd90aa",
   "metadata": {},
   "source": [
    "There are two sets of low-level APIs: \n",
    "- there is one for manipulating distributed data (RDDs), and\n",
    "- another for distributing and manipulating distributed shared variables (broadcast variables and accumulators)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacee6c",
   "metadata": {},
   "source": [
    "**When to Use the Low-Level APIs?**\n",
    "- When we need some functionality that we cannot find in higer level APIs\n",
    "- When we need to maintain legacy codebase written using RDDs\n",
    "- When we need to do some custom shared variable manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a941d6e",
   "metadata": {},
   "source": [
    "**How to Use the Low-Level APIs?**\n",
    "- A `SparkContext` is the entry point for low-level API functionality.\n",
    "- We can access it through the `SparkSession`, which is the tool we use to perform computation across a Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9217417",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sp.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e25bc9",
   "metadata": {},
   "source": [
    "## About RDDs\n",
    "\n",
    "- RDD represents an immutable, partitioned collection of records that can be operated on in parallel, unlike `DataFrame` through where each record is a structured row containing fields with known schema, in RDDs the records are just JAVA, Scala, or Python objects of the programmer's choosing.\n",
    "- RDDs give us complete control because every record in an RDD is a just a Java or Python object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbd653",
   "metadata": {},
   "source": [
    "## Types of RDDs\n",
    "As a user, we will likely only be creating two types of RDDs:\n",
    "- The generic \n",
    "- key-value i.e, that provieds additional functions, such as aggregating by key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86774c",
   "metadata": {},
   "source": [
    "**each RDD is characterized by five main properties:**\n",
    "- A list of partitions\n",
    "- A function for computing each split\n",
    "- A list of dependencies on other RDDs\n",
    "- Optionally, a list of preferred locations on which to compute each split(e.g., block locations for a Hadoop Distributed File System [HDFS] file)\n",
    "- Optionally, a `partitioner` for key-value RDDs (e.g., to say that the RDD is hash-partitioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0aaf3",
   "metadata": {},
   "source": [
    "**For Scala and Java, the performance is for the most part the same, the large costs incurred in manipulating the raw objects but in python can lose substantial amount of performance when using RDDs.**\n",
    "\n",
    "Running Python RDDs equates to running Python user-defined functions (UDFs) row by row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131b6a4",
   "metadata": {},
   "source": [
    "### When to use RDDs ?\n",
    "The most likely reason for use RDDs is because we need fine-grained control over the physical distribution of data (custom partitioning of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa8d27",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
