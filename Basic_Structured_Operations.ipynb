{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Structured Operations.ipynb",
      "provenance": [],
      "mount_file_id": "1aAA9-5P8Cs5XQ1Su_tDOxdtQ3xTHXSuq",
      "authorship_tag": "ABX9TyPPyHolbhmevIOHhMz8yJJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhashuva/pyspark_tutorial/blob/main/Basic_Structured_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_yIQejz18cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3870f0f6-5b7c-4663-de2e-e59863d28bf7"
      },
      "source": [
        "% pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 63 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 79.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=e8b3b95ac0de596c80cebe5f2714fb7c453af2ab56d49df3bd5d5ecb3f31e27f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_bUF-9oQgyl"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.session import SparkSession"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNXJKmZIRAq1"
      },
      "source": [
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Basic Structured operations\")\n",
        "sc = SparkContext(conf= conf)\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX8sUGO7R1BO"
      },
      "source": [
        "df = spark.read.format(\"json\").load(\"/content/drive/MyDrive/data for spark/2015-summary.json\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJXxsdRBVSnv",
        "outputId": "842c15d1-acb2-488e-888b-0f7f56c8f9a8"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t2UzqPeWmXC",
        "outputId": "092f12b6-cc7c-47e3-c8b5-668fb1b7ed7a"
      },
      "source": [
        "spark.read.format(\"json\").load(\"/content/drive/MyDrive/data for spark/2015-summary.json\").schema"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-P0VUIXNkK"
      },
      "source": [
        "- The example that follows shows how to create and enforce a specific schema on DataFrame,\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxHQArW7WxG2"
      },
      "source": [
        "from pyspark.sql.types import StructField, StructType, LongType, StringType"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m4td2yCX9da"
      },
      "source": [
        "myManualSchema = StructType([\n",
        "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2d68sp9YU1D"
      },
      "source": [
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        ".load(\"/content/drive/MyDrive/data for spark/2015-summary.json\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5weH-kGvavaq"
      },
      "source": [
        "# Columns and Expressions\n",
        "To spark, columns are logical constructions that simply represent a value computed on a per-record basis by means of an expression.\n",
        "\n",
        "## Columns\n",
        "There are a lot of different ways to construct and refer to coulmns but the two simplest ways are by using the ```column``` or ```col``` functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpo89f86YjtS",
        "outputId": "25da5bba-457b-4d70-f26c-b093270262fc"
      },
      "source": [
        "from pyspark.sql.functions import col,column\n",
        "col(\"someColumnName\")\n",
        "col(\"someColumnName\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'someColumnName'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rpq6lc0i6-7"
      },
      "source": [
        "### Columns as expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pBPc3nJcqev",
        "outputId": "e8f456fd-e1dc-4de9-e3b4-979af65531c2"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import expr\n",
        "expr(\"(((somecol+5)*200)-6)< otherCol\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'((((somecol + 5) * 200) - 6) < otherCol)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMhXHepXjElM"
      },
      "source": [
        "### Accessing a DataFrame's Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHAbuMdpiz9u",
        "outputId": "69211454-aa1a-4ccb-d07b-68e83d29590d"
      },
      "source": [
        "spark.read.format(\"json\").load(\"/content/drive/MyDrive/data for spark/2015-summary.json\").columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkcyK9ofnnZ-",
        "outputId": "5451e385-3545-44ee-a7d3-b54bf574f907"
      },
      "source": [
        "df.first()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJ0Jv1ij1Ya"
      },
      "source": [
        "# Records and Rows\n",
        "\n",
        "You can creatte rows by manually instantiating a Row object with the values that belong in each column. \n",
        "- It is important to note that only DataFrame have schemas. Rows themselves do not have schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJ92MRpjTY2"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "myRow = Row(\"Hello\",None,1,False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wbs7xi6hnbLu",
        "outputId": "036f736b-a1ac-4d0c-beee-bd2982d49fd5"
      },
      "source": [
        "myRow[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAtyPEznn3DS"
      },
      "source": [
        "myRow[1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLEts-3Pn46X",
        "outputId": "23d9b2b3-0a20-49cc-9d28-d8e72ff349a8"
      },
      "source": [
        "myRow[2]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YGfDkASoN5f"
      },
      "source": [
        "# DataFrame Transformations \n",
        "- add rows or columns\n",
        "- remove rows or columns\n",
        "- transform a row into a column (or vice versa)\n",
        "- Change the order of rows based on the values in columns.\n",
        "\n",
        "## Creating DataFrames\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf7T23Zgn8LB"
      },
      "source": [
        "df = spark.read.format(\"json\").load(\"/content/drive/MyDrive/data for spark/2015-summary.json\")\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYCNKw7BpxSC"
      },
      "source": [
        "We can create DataFrames on the fly by taking a set of rows and converting them to a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo1km8BhplFt"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "                             StructField(\"some\",StringType(),True),\n",
        "                             StructField(\"col\", StringType(), True),\n",
        "                             StructField(\"names\",LongType(), False)\n",
        "])\n",
        "myRow = Row(\"Hello\", None, 1)\n",
        "myDf = spark.createDataFrame([myRow],myManualSchema)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gfqpm2ytJSO",
        "outputId": "a1104054-9404-46e3-d406-69912821d64e"
      },
      "source": [
        "myDf.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----+-----+\n",
            "| some| col|names|\n",
            "+-----+----+-----+\n",
            "|Hello|null|    1|\n",
            "+-----+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETyTbW9uvg6s"
      },
      "source": [
        "## select and selectExpr\n",
        "\n",
        "- The ```select``` method is used when we working with columns or expressions.\n",
        "\n",
        "- The ```selectExpr``` method when we work with expressions in strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT4ivC4WtL4w",
        "outputId": "f91b10d3-87ea-44c7-e52f-5089b76652d3"
      },
      "source": [
        "%%sql\n",
        "SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%sql` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvoAjIymwrC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aef124-5330-428f-d389-2efbd5a7a464"
      },
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\").show(2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|    United States|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ3vXpjcGmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf6a02e-701c-495f-8f5a-eb5dabf068cc"
      },
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show(2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "+-----------------+-------------------+\n",
            "|    United States|            Romania|\n",
            "|    United States|            Croatia|\n",
            "+-----------------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Tu3M0tdAt_"
      },
      "source": [
        "One common error is attempting to mix Column objects and strings. For example, the following code will result in compile error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INW4hg_5c3c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf60145-d6c6-4833-b987-8fb3f1fe6d24"
      },
      "source": [
        "df.select(col(\"DEST_COUNTRY_NAME\"),\"DEST_COUNTRY_NAME\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, DEST_COUNTRY_NAME: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsJgLIoeMaO"
      },
      "source": [
        "```expr``` is the most flexible reference that we can use. It can refer to plain column or a string manipulation of a column. To illustrate, lets change the column name, and then change it back by using the AS keyword and then the alias method on the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkaMLlyAdynE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23387834-7978-4b6c-e444-87567816f706"
      },
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|  destination|\n",
            "+-------------+\n",
            "|United States|\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwEdUno2fRMC"
      },
      "source": [
        "This changes the column name to \"destination\". You can further manipulate the result of your expression as another expression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNWlmLkLfKyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cc874b-51c1-4d12-e54f-20fffe075d30"
      },
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|    United States|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI2XCn0tgBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe885e67-d712-47e2-b22b-0f777a3f6567"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2jX-9O-jXlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edfdf9c-79a2-4d68-c7ee-0989177fbaf1"
      },
      "source": [
        "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----------------+\n",
            "|newColumnName|DEST_COUNTRY_NAME|\n",
            "+-------------+-----------------+\n",
            "|United States|    United States|\n",
            "|United States|    United States|\n",
            "+-------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04KAQq4qJ5G_",
        "outputId": "1b82e9ce-d902-4b76-ff67-feafb7107f17"
      },
      "source": [
        "df.selectExpr(\n",
        "    \"*\",# all original columns\n",
        "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCOuntry\").show(2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCOuntry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZYd0WTpKxYc"
      },
      "source": [
        "With select expression, we can also specify aggregations over the entire DataFrame by taking advantage of the functions that we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhzheIpBKjNG",
        "outputId": "99b968fa-6548-4bdb-e53c-e90363700e55"
      },
      "source": [
        "df.selectExpr(\"avg(count)\",\"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------------------------------+\n",
            "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
            "+-----------+---------------------------------+\n",
            "|1770.765625|                              132|\n",
            "+-----------+---------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpXYJT_XLmfE"
      },
      "source": [
        "## Converting to Spark Types (Literals)\n",
        "- Some times we need to pass explicit values into the spark that are just a value that might be constant value or something that needs to compare later on. One way to do this is through ```literlas```. \n",
        "- That is basically a translation from a given programming language's literal value to one that spark understands.\n",
        "\n",
        "- Literals are expressions and we can use them in same way we use expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-bJuPszLcua",
        "outputId": "b002ab41-4e97-4d37-c6e7-d13195beed43"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
            "+-----------------+-------------------+-----+---+\n",
            "|    United States|            Romania|   15|  1|\n",
            "|    United States|            Croatia|    1|  1|\n",
            "+-----------------+-------------------+-----+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBUSjGk1OavC"
      },
      "source": [
        "## Adding Columns\n",
        "Using the ```withColumn``` method we can add new column to the data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S_gn9DNN_jv",
        "outputId": "fe3ba04b-9ab2-4be1-a3d4-444d99352cd1"
      },
      "source": [
        "df.withColumn(\"numberOne\",lit(1)).show(2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "|    United States|            Romania|   15|        1|\n",
            "|    United States|            Croatia|    1|        1|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7HtXJxvRYw8",
        "outputId": "06a66101-71cb-4beb-cf23-57c0217de8c3"
      },
      "source": [
        "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME==DEST_COUNTRY_NAME\")).show(2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBQai6uaStZ9"
      },
      "source": [
        "- ```withColumn``` function takes two arguments: the column name and the expression that will create the value for the given row in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szeBp-33RuU7",
        "outputId": "4db8cd03-6a4b-4c53-cbec-636dda6b8b4c"
      },
      "source": [
        "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tws7ihUPTa-D"
      },
      "source": [
        "## Renaming Columns\n",
        "We can rename the columns with the help of ```ColumnRenamed``` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC7sakG6TN1u",
        "outputId": "15c5a426-758f-47ab-8deb-adaa36140523"
      },
      "source": [
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36bIhogUpR_"
      },
      "source": [
        "## Reserved Characters and Keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biMrY3O8Um0Z"
      },
      "source": [
        "dfWithLongColName = df.withColumn(\n",
        "    \"This Long Column-Name\",\n",
        "    expr(\"ORIGIN_COUNTRY_NAME\")\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "vBK2caX0VhYf",
        "outputId": "1b740ad1-e4bf-4d55-9f5c-3de299b1956c"
      },
      "source": [
        "dfWithLongColName.selectExpr(\n",
        "    \"`This Long Column-Name`\",\n",
        "    \"`This Long Column-Name` as 'new col'\").show(2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParseException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d7d27ec8ed37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dfWithLongColName.selectExpr(\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"`This Long Column-Name`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \"`This Long Column-Name` as 'new col'\").show(2)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParseException\u001b[0m: \nextraneous input ''new col'' expecting <EOF>(line 1, pos 27)\n\n== SQL ==\n`This Long Column-Name` as 'new col'\n---------------------------^^^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43waCAOlV7DE"
      },
      "source": [
        "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "i-dKyacU2b1f",
        "outputId": "c2e78a17-2707-4df3-8c68-385606beb7e2"
      },
      "source": [
        "--in SQL\n",
        "SELECT 'This Long Column-Name', 'This Long COlumn-Name' as 'new col' FROM dfTableLong LIMIT 2"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-8c992336cd7a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --in SQL\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKlGI_DM3DSc"
      },
      "source": [
        "## Case Sensitivity\n",
        "\n",
        "- By default Spark is case insensitive; however, we can make Spark case sensitive by setting the configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "Th3riT4k3Big",
        "outputId": "e5b9281f-9610-4570-c8e7-2b9647ec70d7"
      },
      "source": [
        "--in SQL\n",
        "set spark.sql.caseSensitive true"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-c0766426f5c0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --in SQL\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdYlfocB3-zl"
      },
      "source": [
        "## Removing Columns\n",
        "\n",
        "- There is dedicated method called ```drop```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmrpHXJ44Yvy",
        "outputId": "fe1e3499-b3fe-4dfa-8550-00595978fe61"
      },
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAR3wtv14iBm"
      },
      "source": [
        "- We can drop multiple columns by passing multiple columns as arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQmn_Kd84f1y",
        "outputId": "97ed14af-4512-4cc0-af06-58dbab924e87"
      },
      "source": [
        "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[count: bigint, This Long Column-Name: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djCQoKDe5A4-"
      },
      "source": [
        "### Changing a Column's Type(cast)\n",
        "- We can convert columns from one type to another by casting the column from one type to another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSKjEBZY4-EI",
        "outputId": "16a60bbc-2c08-4364-eb9f-c36d318c84ee"
      },
      "source": [
        "df.withColumn(\"count2\",col(\"count\").cast(\"long\"))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "TdmrEqygCkzd",
        "outputId": "cff1d17b-9f4c-4b49-8547-5ae2c7abf002"
      },
      "source": [
        "-- in SQL\n",
        "SELECT *, cast(count as long) AS count2 FROM dfTable"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-a40e225fb135>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -- in SQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhPNUoyfC12P"
      },
      "source": [
        "### Filtering Rows\n",
        "- To filter rows, we create an expression that evalutes to true or false.\n",
        "- You then filter out the rows with an expression that is equal to false.\n",
        "\n",
        "- There are two methods to perform this operation: We can use ```where``` or ```filter``` and they both will perform the same operation and accept the same argument types when used with DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzSwaoKHkZo"
      },
      "source": [
        "NOTE: When using the Dataset API from either scala or Java, filter also accepts an arbitrary function that Spark will apply to each record in the Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIxJmMEpHgow",
        "outputId": "103f5948-939d-4607-f763-6a3bd5a573b1"
      },
      "source": [
        "df.filter(col(\"count\")<2).show(2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uDyWv1NQTXK",
        "outputId": "1dc38447-60a4-46b5-f62c-93b2ca938e45"
      },
      "source": [
        "df.where(\"count<2\").show(2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "V2ThJE0vQopt",
        "outputId": "9c4bd439-7d43-4900-da97-2767e38a37c1"
      },
      "source": [
        "-- in SQL\n",
        "SELECT * FROM dfTable WHERE count < 2 LIMIT 2"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-4c557abf71be>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -- in SQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISW2fwffRzak"
      },
      "source": [
        "Spark automatically performs all filtering operations at the same time regardless of the filter ordering. \n",
        "- If we want to specify multiple AND filters, chain them sequentially and let Spark handle the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV2NCHyXQ164",
        "outputId": "975b9567-6db9-4f63-aa8f-09bd547fcc13"
      },
      "source": [
        "# in python\n",
        "df.where(col(\"count\")<2).where(col(\"ORIGIN_COUNTRY_NAME\")!= \"Croatia\").show(2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|          Singapore|    1|\n",
            "|          Moldova|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "wi-y93fLS7K6",
        "outputId": "05aa0c65-7084-4198-e7e4-eea60b4b00a2"
      },
      "source": [
        "--in SQL\n",
        "SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\" LIMIT 2"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-28c07af150e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --in SQL\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WfqkFifVWxl"
      },
      "source": [
        "### Getting Unique Rows\n",
        "BY using the ```distinct``` method on a DataFrame, which allows us to deduplicate any rows that are in that DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tP0a8GGVWRs",
        "outputId": "a19c7704-0d40-4708-f95c-3443f22eb8c2"
      },
      "source": [
        "# in python\n",
        "df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().count()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "ErjF7vzYW2KB",
        "outputId": "ac561941-82c0-4c9e-f9d3-f372a664a0b7"
      },
      "source": [
        "-- in SQL\n",
        "SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-d8e926de1bed>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -- in SQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6eqeUwyXawr",
        "outputId": "e0f454dd-bcf4-4358-b84f-56847653e8d0"
      },
      "source": [
        "# in python\n",
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "oonJ9Jm2XnA9",
        "outputId": "f9d8e391-70fc-45b1-b9a4-3ea825cfffda"
      },
      "source": [
        "--inSQL\n",
        "SELECT COUNT(DISTINCT ORIGIN_COUTNRY_NAME) FROM dfTable"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-a7ba96d2e3ab>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    SELECT COUNT(DISTINCT ORIGIN_COUTNRY_NAME) FROM dfTable\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iF01nOhY5RO"
      },
      "source": [
        "### Random Samples\n",
        "If we want to sample some random records we can do this by using the ```sample``` method on a DataFrame to specify a fraction of rows to extract from a DataFrame and whether we would like to sample with or without replacement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmEpWedbZyZf",
        "outputId": "285073b8-a266-43c9-f8eb-a3bc1a72d737"
      },
      "source": [
        "# in python\n",
        "seed = 5\n",
        "withReplacement = False\n",
        "fraction = 0.5\n",
        "df.sample(withReplacement, fraction, seed).count()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rhYE0WWaLw8"
      },
      "source": [
        "### Concatenating and Appending Rows(Union)\n",
        "- To append to a DataFrame, we must ```union``` the original DataFrame along with the new DataFrame, that will concatenates the two DataFrames.\n",
        "- To ```union``` two DataFrames, we must be sure that they have the same schema and number of columns; otherwise the union will fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G46xBd04HBiW"
      },
      "source": [
        "**WARNING**: Unions are currently performed based on location, not on the schema. This means that columns will not automatically line up the way we think."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hmW3MRyaCtf"
      },
      "source": [
        "# in python\n",
        "from pyspark.sql import Row\n",
        "schema = df.schema\n",
        "newRows = [\n",
        "           Row(\"New Country\", \"Other Country\", 5),\n",
        "           Row(\"New Country 2\", \"Other Country 3\", 1)\n",
        "]\n",
        "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
        "newDF = spark.createDataFrame(parallelizedRows, schema)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdDtcYKEItwD",
        "outputId": "63260872-2945-4b06-9539-2a4c61f1194f"
      },
      "source": [
        "df.union(newDF).where(\"count=1\").where(col(\"ORIGIN_COUNTRY_NAME\")!=\"United States\").show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "|    United States|          Gibraltar|    1|\n",
            "|    United States|             Cyprus|    1|\n",
            "|    United States|            Estonia|    1|\n",
            "|    United States|          Lithuania|    1|\n",
            "|    United States|           Bulgaria|    1|\n",
            "|    United States|            Georgia|    1|\n",
            "|    United States|            Bahrain|    1|\n",
            "|    United States|   Papua New Guinea|    1|\n",
            "|    United States|         Montenegro|    1|\n",
            "|    United States|            Namibia|    1|\n",
            "|    New Country 2|    Other Country 3|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM8GcAAyIrZG"
      },
      "source": [
        "### Sorting Rows\n",
        "When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame.\n",
        "\n",
        "- There are two equivalent operations to do this ```sort``` and ```orderBy``` that work the exact same way.\n",
        "\n",
        "- They accept both column expressions and strings as well as multiple columns. \n",
        "\n",
        "- The default is to sort in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC5EoC2FJxMd",
        "outputId": "733adda1-3b7b-4a23-ad86-5a4adb070ef7"
      },
      "source": [
        "# in python\n",
        "df.sort(\"count\").show(5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|               Malta|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "|       United States|          Singapore|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8_sZKgfJ0Ls"
      },
      "source": [
        "- To more explicitly specify sort direction, you need to use the ```asc``` and ```desc``` functions if operating on a column. These allow you to specify the order in which s given column should be sorted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zSfwNLYJvnO",
        "outputId": "10cd56b1-c86b-426e-a96e-9e7b5b1a7597"
      },
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\")).show(3)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|          Moldova|      United States|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "|    United States|            Croatia|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_RkbNRQLYD9",
        "outputId": "8e5bb756-5cdf-4cec-ee8c-669cf1849c0f"
      },
      "source": [
        "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
            "+-----------------+-------------------+------+\n",
            "|    United States|      United States|370002|\n",
            "|    United States|             Canada|  8483|\n",
            "+-----------------+-------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "JrnYgceHLzo7",
        "outputId": "9b875fee-5538-44e4-9b73-7a2a25d85436"
      },
      "source": [
        "--in SQL\n",
        "SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-a27222304aeb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --in SQL\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTymyDALNH1P"
      },
      "source": [
        "- An advanced tip is to use ```asc_nulls_first```,```desc_nulls_first```,```asc_nulls_last```, or ```desc_nulls_last``` to specify where we would like we null values to appear in an ordered DataFrame.\n",
        "\n",
        "- For optimization purposes, it's sometimes advisable to sort within each partition before another set of transformations. We can use the ```sortWithinPartitions``` method to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "a4_AYCl1NHUn",
        "outputId": "404e76c9-8528-4721-d346-6c7b6dde57e4"
      },
      "source": [
        "# in python\n",
        "spark.read.format(\"json\").load(\"/data/flight-data/json/*-summary.json\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-aceceef5ad99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# in python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/flight-data/json/*-summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/data/flight-data/json/*-summary.json"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAD4sBZOx3Y"
      },
      "source": [
        "### Limit\n",
        "We might to restrict what we extract from a DataFrame: for example, we might want just the top ten of some DataFrame. We can do this by using the ```limit``` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgATKFKoPitG",
        "outputId": "3052a9f0-8dde-469b-dbd6-4856b970a46f"
      },
      "source": [
        "df.limit(5).show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "FG9y4FIjPvRN",
        "outputId": "026b0ce9-32d4-4a8f-e379-0c3077efd320"
      },
      "source": [
        "-- in SQL\n",
        "SELECT * FROM dfTable LIMIT 6"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-3dc1c88ad26a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -- in SQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYYzPeKbP4xF",
        "outputId": "f56518ba-eb39-4c7f-df08-e9e42c6e6d05"
      },
      "source": [
        "df.orderBy(expr(\"count desc\")).limit(6).show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|               Malta|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "|       United States|          Singapore|    1|\n",
            "|             Moldova|      United States|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "RDlrC-xCQFTL",
        "outputId": "b9379eb1-d8a1-428f-e573-35540728c112"
      },
      "source": [
        "-- in SQL\n",
        "SELECT * FROM dfTable ORDER BY count desc LIMIT 6"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-9b64d423968b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -- in SQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziAAOZcIzjCs"
      },
      "source": [
        "### Repartioning and coalsec\n",
        "- One more important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partition scheme and the number of partitions.\n",
        "\n",
        "- Repartio will incur a full shuffle of the data, regardless of whether one is necessary. This means we should typically only repartition when the future number of partitions is greater than our current number of partitions or when we are looking to partition by a set of columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySTvYcpf1X6o",
        "outputId": "ee48727a-79ff-422a-ba32-2a0aaf18c4c8"
      },
      "source": [
        "#in python\n",
        "df.rdd.getNumPartitions()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQY1i27e1kUW",
        "outputId": "96b8b4f7-303d-4d25-993f-4aa49044ab26"
      },
      "source": [
        "df.repartition(5)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKl4rEYZ2ueP"
      },
      "source": [
        "If we know that we are going to be filtering by a certain column often, it can be worth repartioning based on the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r68MkwAs2rQE",
        "outputId": "25157564-a049-4d56-8fdf-c822170a277e"
      },
      "source": [
        "#python\n",
        "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvDrSF3r3THV"
      },
      "source": [
        "- We can optionally specify the number of partitions we would like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upxRgQp23QQR",
        "outputId": "47af9266-a5a1-42e2-adb6-3bf71321a27c"
      },
      "source": [
        "# in python\n",
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJhS43gP7RfT"
      },
      "source": [
        "- Coalsec will not incur a full shuffle and will try to combine partitions. This operation will shuffle our data into five partitions based on the destination on country name and then coalsec them(without a full shuffle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgztDChv7OeY",
        "outputId": "26e25f1e-e814-4d78-ba47-132e1236e1f8"
      },
      "source": [
        "#in python\n",
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjk8gpE98pUb"
      },
      "source": [
        "### Collecting Rows to the Driver\n",
        "- Spark maintains the state of the cluster in the driver.\n",
        "- There will be times when we want to collect some of our data to the driver in order to manipulate it on our local machine.\n",
        "- ```collect``` gets all data from the entire DataFrame, take selects the first *N* rows, and show prints out a number of rows, nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xlCxXiR8Ueg",
        "outputId": "814c746f-f2d5-4d83-b933-b11c164a9d77"
      },
      "source": [
        "# in python\n",
        "collectDF = df.limit(10)\n",
        "collectDF.take(5) #take works with an Integer count\n",
        "collectDF.show() #this prints it out nicely\n",
        "collectDF.show(5, False)\n",
        "collectDF.collect()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "|    United States|          Singapore|    1|\n",
            "|    United States|            Grenada|   62|\n",
            "|       Costa Rica|      United States|  588|\n",
            "|          Senegal|      United States|   40|\n",
            "|          Moldova|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|United States    |Romania            |15   |\n",
            "|United States    |Croatia            |1    |\n",
            "|United States    |Ireland            |344  |\n",
            "|Egypt            |United States      |15   |\n",
            "|United States    |India              |62   |\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
              " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
              " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
              " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0zLFHsQ_zB1"
      },
      "source": [
        "There's an additional way of collecting rows to the driver in order to iterate over the entire dataset.\n",
        "- The method ```toLocalIterator``` collects partitions to the driver as an iterator.\n",
        "\n",
        "- This method allows us to iterate over the entire dataset partition-by-partition in a serial manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAiK6VqI8mDJ",
        "outputId": "e7391353-c771-40f6-c708-c1c1089e8768"
      },
      "source": [
        "collectDF.toLocalIterator()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7ff13ee6ba50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC4Lot0ABsk1"
      },
      "source": [
        "**WARNING:** Any collection of data to the driver can be a very expensive operation. If we have a large dataset and collect,we can crash the driver. If we use localIterator and have very large partitions. We can easily crash the driver node and lose the state of our application. This is also expensive because we can operate on a one-by-one basis, instead of running computation in parallel.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN1hF3nGBpNI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}